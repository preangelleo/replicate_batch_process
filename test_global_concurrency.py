#!/usr/bin/env python3
"""
æµ‹è¯•å…¨å±€å¹¶å‘æ§åˆ¶å‡çº§
Test Global Concurrency Control Upgrade

éªŒè¯å…¨å±€å¹¶å‘æ§åˆ¶åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import os
import time
import sys
from typing import List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'replicate_batch_process'))

from replicate_batch_process.intelligent_batch_processor import intelligent_batch_process
from replicate_batch_process.global_concurrency_manager import create_global_manager, get_global_status


async def test_global_concurrency_manager():
    """æµ‹è¯•å…¨å±€å¹¶å‘ç®¡ç†å™¨åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•1: å…¨å±€å¹¶å‘ç®¡ç†å™¨åŸºæœ¬åŠŸèƒ½")
    
    # ä½¿ç”¨æµ‹è¯•å‡­æ®åˆ›å»ºç®¡ç†å™¨
    test_api_token = "r8_test_token_12345678901234567890"
    manager = create_global_manager(
        api_token=test_api_token,
        global_max_concurrent=3
    )
    
    # è·å–çŠ¶æ€
    status = get_global_status()
    print(f"ğŸ“Š å…¨å±€çŠ¶æ€: {status}")
    
    # æµ‹è¯•ä¿¡å·é‡è·å–å’Œé‡Šæ”¾
    print("\nğŸ”„ æµ‹è¯•ä¿¡å·é‡è·å–å’Œé‡Šæ”¾...")
    semaphore = manager.get_global_semaphore()
    
    async def test_task(task_id: int):
        async with semaphore:
            print(f"  ä»»åŠ¡ {task_id} è·å¾—å…¨å±€é…é¢")
            await asyncio.sleep(1)  # æ¨¡æ‹ŸAPIè°ƒç”¨
            print(f"  ä»»åŠ¡ {task_id} å®Œæˆ")
    
    # å¯åŠ¨å¤šä¸ªä»»åŠ¡æµ‹è¯•å¹¶å‘æ§åˆ¶
    tasks = [test_task(i) for i in range(6)]
    start_time = time.time()
    await asyncio.gather(*tasks)
    duration = time.time() - start_time
    
    print(f"âœ… 6ä¸ªä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {duration:.1f}ç§’")
    print(f"   é¢„æœŸ: çº¦ 2ç§’ (3å¹¶å‘ï¼Œæ¯ä»»åŠ¡1ç§’)")
    
    # æœ€ç»ˆçŠ¶æ€
    final_status = get_global_status()
    print(f"ğŸ“Š æœ€ç»ˆçŠ¶æ€: {final_status}")


async def test_environment_priority():
    """æµ‹è¯•ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§ç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•2: å•ä¾‹æ¨¡å¼è¡Œä¸ºéªŒè¯")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["REPLICATE_API_TOKEN"] = "r8_env_token_123"
    os.environ["REPLICATE_GLOBAL_MAX_CONCURRENT"] = "5"
    
    # ç”±äºå•ä¾‹æ¨¡å¼ï¼Œæ‰€æœ‰ç®¡ç†å™¨å®ä¾‹éƒ½å…±äº«ç›¸åŒçš„å…¨å±€çŠ¶æ€
    print("ğŸ”‘ éªŒè¯å•ä¾‹æ¨¡å¼è¡Œä¸º...")
    manager1 = create_global_manager(
        api_token="r8_payload_token_456",
        global_max_concurrent=8
    )
    
    manager2 = create_global_manager()  # ä¸ä¼ å…¥ä»»ä½•å‚æ•°
    
    # éªŒè¯å•ä¾‹è¡Œä¸ºï¼šä¸¤ä¸ªç®¡ç†å™¨å®é™…ä¸Šæ˜¯åŒä¸€ä¸ªå®ä¾‹
    assert manager1 is manager2, "å•ä¾‹æ¨¡å¼å¤±è´¥"
    print("âœ… å•ä¾‹æ¨¡å¼æ­£å¸¸ï¼šæ‰€æœ‰å®ä¾‹å…±äº«å…¨å±€çŠ¶æ€")
    
    status1 = manager1.get_global_status()
    status2 = manager2.get_global_status()
    
    print(f"ğŸ“Š ç®¡ç†å™¨1çŠ¶æ€: max_concurrent={status1['global_max_concurrent']}")
    print(f"ğŸ“Š ç®¡ç†å™¨2çŠ¶æ€: max_concurrent={status2['global_max_concurrent']}")
    
    # éªŒè¯å…³é”®çŠ¶æ€å­—æ®µä¸€è‡´ï¼ˆå¿½ç•¥æ—¶é—´ç›¸å…³å­—æ®µï¼‰
    key_fields = ['global_max_concurrent', 'current_concurrent', 'available_slots', 'utilization_percentage']
    for field in key_fields:
        assert status1[field] == status2[field], f"å­—æ®µ {field} ä¸ä¸€è‡´: {status1[field]} != {status2[field]}"
    print("âœ… å…¨å±€çŠ¶æ€ä¸€è‡´æ€§æ­£å¸¸")


async def test_batch_processing_integration():
    """æµ‹è¯•æ‰¹å¤„ç†é›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•3: æ‰¹å¤„ç†é›†æˆ (æ¨¡æ‹Ÿï¼Œä¸å®é™…è°ƒç”¨API)")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_prompts = [
        f"Test image {i+1}: beautiful landscape"
        for i in range(5)
    ]
    
    print(f"ğŸ“ å‡†å¤‡ {len(test_prompts)} ä¸ªæµ‹è¯•prompt")
    
    # ä½¿ç”¨å…¨å±€å¹¶å‘æ§åˆ¶çš„æ‰¹å¤„ç†
    print("ğŸš€ å¯åŠ¨å…¨å±€å¹¶å‘æ§åˆ¶çš„æ‰¹å¤„ç†...")
    
    # æ³¨æ„ï¼šè¿™é‡Œä¼šå¤±è´¥ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰çœŸå®çš„API token
    # ä½†å¯ä»¥éªŒè¯å‚æ•°ä¼ é€’æ˜¯å¦æ­£ç¡®
    try:
        files = await intelligent_batch_process(
            prompts=test_prompts,
            model_name="black-forest-labs/flux-dev",
            max_concurrent=3,
            replicate_api_token="r8_test_integration_token",
            global_max_concurrent=2,
            output_dir="/Users/lgg/coding/testing_folder/test_global_concurrency_output"
        )
        print(f"âœ… æ‰¹å¤„ç†å®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶: {len(files)}")
        
    except Exception as e:
        if "api_token" in str(e).lower() or "replicate" in str(e).lower():
            print("âœ… å…¨å±€å¹¶å‘æ§åˆ¶é›†æˆæ­£å¸¸ (é¢„æœŸçš„API tokené”™è¯¯)")
        else:
            print(f"âŒ æ„å¤–é”™è¯¯: {e}")
            raise


async def test_multiple_instances():
    """æµ‹è¯•å¤šå®ä¾‹å…±äº«å…¨å±€çŠ¶æ€"""
    print("\nğŸ§ª æµ‹è¯•4: å¤šå®ä¾‹å…±äº«å…¨å±€çŠ¶æ€")
    
    from replicate_batch_process.intelligent_batch_processor import IntelligentBatchProcessor
    
    # åˆ›å»ºå¤šä¸ªå¤„ç†å™¨å®ä¾‹
    processor1 = IntelligentBatchProcessor(
        max_concurrent=4,
        replicate_api_token="r8_instance1_token",
        global_max_concurrent=6
    )
    
    processor2 = IntelligentBatchProcessor(
        max_concurrent=3,
        replicate_api_token="r8_instance2_token",  # ä¸åŒtokenä½†åº”è¯¥ä½¿ç”¨å…¨å±€ç®¡ç†å™¨
        global_max_concurrent=8  # ä¸åŒè®¾ç½®ä½†åº”è¯¥ä½¿ç”¨å·²å­˜åœ¨çš„å…¨å±€ç®¡ç†å™¨
    )
    
    # éªŒè¯éƒ½ä½¿ç”¨åŒä¸€ä¸ªå…¨å±€ç®¡ç†å™¨
    status1 = processor1.global_manager.get_global_status()
    status2 = processor2.global_manager.get_global_status()
    
    print(f"ğŸ“Š å®ä¾‹1çŠ¶æ€: {status1}")
    print(f"ğŸ“Š å®ä¾‹2çŠ¶æ€: {status2}")
    
    # ç”±äºå•ä¾‹æ¨¡å¼ï¼Œä¸¤ä¸ªå®ä¾‹åº”è¯¥å…±äº«ç›¸åŒçš„å…¨å±€é…ç½®
    assert status1['global_max_concurrent'] == status2['global_max_concurrent'], "å…¨å±€çŠ¶æ€å…±äº«å¤±è´¥"
    print("âœ… å¤šå®ä¾‹å…¨å±€çŠ¶æ€å…±äº«æ­£å¸¸")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•å…¨å±€å¹¶å‘æ§åˆ¶å‡çº§")
    print("="*60)
    
    try:
        await test_global_concurrency_manager()
        await test_environment_priority()
        await test_batch_processing_integration()
        await test_multiple_instances()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å…¨å±€å¹¶å‘æ§åˆ¶å‡çº§æˆåŠŸ")
        
        # æœ€ç»ˆä½¿ç”¨è¯´æ˜
        print("\nğŸ“š ä½¿ç”¨æ–¹å¼:")
        print("1. ç¯å¢ƒå˜é‡æ–¹å¼:")
        print("   export REPLICATE_API_TOKEN='your_token'")
        print("   export REPLICATE_GLOBAL_MAX_CONCURRENT=10")
        print("   await intelligent_batch_process(prompts, model)")
        print()
        print("2. å‚æ•°ä¼ é€’æ–¹å¼ (ä¼˜å…ˆçº§æ›´é«˜):")
        print("   await intelligent_batch_process(")
        print("       prompts=prompts,")
        print("       model_name=model,")
        print("       replicate_api_token='your_token',")
        print("       global_max_concurrent=15")
        print("   )")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())